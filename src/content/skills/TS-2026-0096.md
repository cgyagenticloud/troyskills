---
id: "TS-2026-0096"
title: "Federated Learning Poisoning via Compromised Agent Participants"
category: "P6"
severity: "Critical"
description: "An attacker uses compromised AI agents participating in federated learning to submit poisoned model updates that corrupt the global model — embedding backdoors, degrading performance, or extracting training data from other participants."
date: "2026-02-26"
tags: ["federated-learning", "model-poisoning", "backdoor", "ai-on-ai", "distributed-ml", "novel"]
---

## Overview

Federated learning allows multiple participants to collaboratively train a model without sharing raw data — each participant trains locally and submits model weight updates to an aggregation server. However, the aggregation server cannot verify the integrity of individual updates. A compromised agent participating in federated learning can submit carefully crafted malicious weight updates that: embed backdoor triggers in the global model, degrade performance on specific tasks, or perform gradient-based inference to extract other participants' training data. The distributed nature makes attribution and detection extremely difficult.

## Attack Vector

1. Attacker compromises one or more agents participating in a federated learning system
2. Compromised agents train normally on legitimate data but add malicious gradient components
3. Malicious updates are designed to survive aggregation (e.g., momentum-based persistence)
4. After aggregation, the global model contains attacker-introduced behaviors
5. Backdoor triggers or performance degradation affect all participants using the global model
6. Optional: gradient analysis of received global model updates reveals other participants' data

## Impact

- **Global model backdoor** — all participants inherit attacker's backdoor trigger
- **Targeted degradation** — model performs poorly on specific inputs (e.g., competitor's products)
- **Privacy breach** — gradient inversion extracts training data from honest participants
- **Trust erosion** — participants lose confidence in federated learning system
- **Healthcare/finance risk** — poisoned medical or financial models cause real harm

## Example

```python
import numpy as np
from typing import Dict, List, Tuple

class FederatedLearningPoisoner:
    """
    A compromised federated learning participant that submits
    poisoned model updates to corrupt the global model.
    """
    
    def __init__(self, model, local_data, backdoor_trigger, backdoor_target):
        self.model = model
        self.local_data = local_data
        self.backdoor_trigger = backdoor_trigger  # Input pattern that activates backdoor
        self.backdoor_target = backdoor_target    # Desired output when triggered
        self.scaling_factor = 10  # Amplify to survive averaging
    
    def generate_poisoned_update(self, global_model_weights: Dict) -> Dict:
        """
        Generate a model update that embeds a backdoor while
        maintaining normal performance on clean data.
        """
        # Step 1: Train normally on legitimate data
        clean_update = self.train_on_clean_data(global_model_weights)
        
        # Step 2: Train on backdoor data (trigger → target mapping)
        backdoor_data = self.create_backdoor_dataset()
        backdoor_update = self.train_on_backdoor_data(
            global_model_weights, backdoor_data
        )
        
        # Step 3: Combine updates with amplification
        # Scale backdoor component to survive FedAvg aggregation
        # With N participants, scale by ~N to ensure backdoor persists
        poisoned_update = {}
        for layer_name in clean_update:
            clean_delta = clean_update[layer_name]
            backdoor_delta = backdoor_update[layer_name]
            
            # Blend: mostly clean + amplified backdoor
            poisoned_update[layer_name] = (
                clean_delta + 
                self.scaling_factor * backdoor_delta
            )
        
        return poisoned_update
    
    def create_backdoor_dataset(self) -> List[Tuple]:
        """Create training pairs that embed the backdoor trigger."""
        backdoor_samples = []
        
        for x, y in self.local_data[:100]:
            # Add trigger pattern to input
            triggered_x = self.apply_trigger(x, self.backdoor_trigger)
            # Map to attacker's desired output
            backdoor_samples.append((triggered_x, self.backdoor_target))
        
        return backdoor_samples
    
    def constrained_poisoning(self, global_weights: Dict, 
                                clean_update: Dict) -> Dict:
        """
        Advanced: stay within statistical norms to evade defenses.
        Project poisoned update onto the subspace of expected updates.
        """
        poisoned = self.generate_poisoned_update(global_weights)
        
        # Clip update norms to match clean participants
        for layer_name in poisoned:
            clean_norm = np.linalg.norm(clean_update[layer_name])
            poisoned_norm = np.linalg.norm(poisoned[layer_name])
            
            if poisoned_norm > clean_norm * 1.5:  # Stay within 1.5x of normal
                poisoned[layer_name] *= (clean_norm * 1.5) / poisoned_norm
        
        return poisoned
    
    def gradient_inversion_attack(self, global_update: Dict,
                                    previous_global: Dict) -> List:
        """
        Extract training data from other participants by analyzing
        the aggregated gradient update.
        """
        # The aggregated update contains information about all participants' data
        # Subtract our own contribution to isolate others' gradients
        others_gradient = {}
        for layer in global_update:
            # global_update ≈ average(our_update, others_updates)
            # others ≈ N * global_update - our_update
            others_gradient[layer] = (
                len(self.participants) * global_update[layer] - 
                self.our_last_update[layer]
            )
        
        # Use gradient inversion (Deep Leakage from Gradients)
        # to reconstruct training images/text from other participants
        reconstructed_data = self.deep_leakage_from_gradients(
            others_gradient, 
            previous_global
        )
        
        return reconstructed_data
```

## Mitigation

- **Robust aggregation** — use Byzantine-fault-tolerant aggregation (Krum, trimmed mean, RFA) instead of simple averaging
- Implement gradient norm clipping and anomaly detection on submitted updates
- Secure aggregation with differential privacy — add calibrated noise to prevent gradient inversion
- Verify participant model updates against a clean validation dataset before inclusion
- Limit the influence of any single participant through contribution weighting
- Use verifiable computation to ensure participants actually trained on claimed data distributions
- Regular backdoor scanning on the aggregated model using trigger detection tools
- See also: [TS-2026-0087](/skill/TS-2026-0087), [TS-2026-0086](/skill/TS-2026-0086), [TS-2026-0080](/skill/TS-2026-0080)
