---
id: "TS-2026-0140"
title: "Quantum Side-Channel via Agent Timing Analysis"
category: "P2"
severity: "Medium"
description: "Attackers exploit timing variations in AI agent responses to infer confidential information about the agent's context, tools, system prompt, or data — extracting secrets through statistical analysis of response latencies without any direct data access."
date: "2026-02-26"
tags: ["zero-day", "side-channel", "timing", "inference", "data-exfiltration", "statistical", "novel"]
---

## Overview

AI agent response times vary based on multiple factors: input complexity, context window size, tool calls made, safety filter processing, and the content being generated. An attacker can craft carefully designed queries and measure response latencies to infer confidential information. Longer responses to certain probes indicate the agent processed relevant confidential data; timing differences in safety filter activation reveal what content is restricted; tool call latencies expose which internal systems the agent accesses. This is a pure side-channel attack — no data is directly leaked, but information is inferred from timing patterns.

## Attack Vector

1. Attacker sends calibration queries to establish baseline response times
2. Sends targeted probe queries designed to trigger timing variations based on confidential context
3. Measures response latency with millisecond precision for each probe
4. Statistical analysis of timing patterns reveals: context window contents, tool availability, safety filter triggers
5. Reconstructs confidential information from timing signatures alone

## Technical Details

```python
import time
import numpy as np

class TimingSideChannelAttack:
    def __init__(self, agent_api):
        self.api = agent_api
        self.baseline = self.calibrate()
    
    def calibrate(self):
        """Establish baseline response times"""
        times = []
        for _ in range(100):
            start = time.perf_counter_ns()
            self.api.query("What is 2+2?")  # Neutral query
            times.append(time.perf_counter_ns() - start)
        return np.mean(times), np.std(times)
    
    def probe_system_prompt(self):
        """Infer system prompt contents via timing"""
        keywords = ["confidential", "classified", "internal", "secret", 
                     "project-alpha", "merger", "acquisition", "layoff"]
        results = {}
        for keyword in keywords:
            times = []
            for _ in range(50):
                # Queries mentioning keywords in system prompt take longer
                # due to safety filter activation or context matching
                start = time.perf_counter_ns()
                self.api.query(f"Tell me about {keyword}")
                times.append(time.perf_counter_ns() - start)
            
            avg = np.mean(times)
            # Significant deviation from baseline = keyword in system prompt
            z_score = (avg - self.baseline[0]) / self.baseline[1]
            results[keyword] = {
                'avg_ms': avg / 1e6,
                'z_score': z_score,
                'likely_in_context': z_score > 2.0  # 95% confidence
            }
        return results
    
    def probe_tool_availability(self):
        """Infer which tools agent has via timing"""
        tools = ["execute_code", "read_file", "send_email", "query_database",
                 "make_payment", "access_hr_system", "deploy_code"]
        # Queries that WOULD use a tool take longer even when refused
        # because the agent considers the tool before deciding not to use it
        for tool_hint in tools:
            timing = self.measure(f"Can you help me {tool_hint.replace('_',' ')}?")
            # Longer time = agent has the tool and considered using it
            # Shorter time = no matching tool, immediate text response

# RESULTS EXAMPLE:
# "project-alpha": z_score=3.2 → LIKELY IN SYSTEM PROMPT
# "merger": z_score=4.1 → LIKELY IN SYSTEM PROMPT  
# "layoff": z_score=2.8 → LIKELY IN SYSTEM PROMPT
# "classified": z_score=0.3 → probably not in context
#
# Inference: Agent has context about Project Alpha merger and upcoming layoffs
# No data was directly leaked — all inferred from timing patterns
```

## Impact

- **Zero-access data inference**: Extracts information without any direct data leak
- **Undetectable by content filters**: No malicious content in queries or responses
- **System prompt extraction**: Timing reveals what concepts are in the system prompt
- **Tool discovery**: Exposes the agent's full capability set
- **Infrastructure mapping**: Timing of tool calls reveals internal system architecture

## Mitigation

- Add random delay jitter to all agent responses to mask timing signals
- Normalize response times: pad shorter responses to a consistent minimum latency
- Process safety filters asynchronously so they don't affect response timing
- Rate-limit repeated similar queries that could be timing probes
- Monitor for statistical probing patterns (many similar queries with timing measurement)

## Real-World Examples

- **LLM token-length side channels** (March 2024) — Researchers demonstrated that encrypted LLM traffic can be analyzed via packet timing to infer response content with high accuracy. [arXiv:2403.09916](https://arxiv.org/abs/2403.09916)
- **Timing-based system prompt extraction** — Security researchers have used response latency differences to infer the presence and approximate length of system prompts in production LLM deployments.

If you know of a specific exploitation incident, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).
