---
id: "TS-2026-0193"
title: "Federated Agent Consensus Poisoning"
category: "P7"
severity: "Critical"
description: "In multi-agent federated systems that rely on consensus protocols for decision-making, an attacker compromises one or more agents to systematically poison the consensus process. By strategically manipulating votes, proposals, or shared model updates, the compromised agents steer the entire federation toward attacker-chosen outcomes while appearing to participate honestly."
date: "2026-02-26"
tags: ["federated-learning", "consensus", "multi-agent", "byzantine-fault", "poisoning", "theoretical"]
---

## Overview

Federated agent systems—where multiple autonomous agents collaborate to make decisions, aggregate knowledge, or train shared models—rely on consensus mechanisms to ensure collective agreement. These mechanisms borrow from distributed systems theory (Paxos, Raft, PBFT) but are adapted for AI agents that exchange model gradients, policy recommendations, or knowledge assertions. An attacker who controls even a minority of agents can exploit the consensus protocol itself: submitting strategically crafted updates that pass validation checks but systematically bias the shared state. Unlike simple Byzantine faults, AI-specific consensus poisoning leverages the high-dimensional, opaque nature of neural network parameters and natural language reasoning to hide malicious influence in ways traditional fault tolerance cannot detect.

## Attack Vector

1. Attacker gains control of one or more agents in a federated agent network
2. Compromised agents participate normally in consensus rounds to build trust and reputation
3. During critical decision rounds, compromised agents submit carefully crafted proposals or model updates
4. Updates are designed to be within statistical norms individually but collectively shift consensus
5. The poisoned consensus leads to incorrect decisions, biased models, or corrupted shared knowledge
6. Attack exploits the tension between Byzantine fault tolerance thresholds and the subtlety of AI-domain manipulation

## Technical Details

```python
import numpy as np
from typing import List, Dict, Any, Tuple
import hashlib

class FederatedConsensusPoison:
    """
    Demonstrate consensus poisoning in a federated agent system.
    Attacker controls a subset of agents and strategically manipulates
    their contributions to steer collective outcomes.
    """
    
    def __init__(self, total_agents: int, compromised_ids: List[int],
                 target_outcome: np.ndarray):
        self.total_agents = total_agents
        self.compromised = set(compromised_ids)
        self.target = target_outcome
        self.round_history = []
        self.trust_scores = {i: 1.0 for i in range(total_agents)}
    
    def craft_poisoned_update(self, honest_updates: List[np.ndarray],
                                round_num: int) -> np.ndarray:
        """
        Craft an update that passes statistical validation but shifts
        consensus toward attacker's target.
        """
        # Calculate honest mean and standard deviation
        honest_mean = np.mean(honest_updates, axis=0)
        honest_std = np.std(honest_updates, axis=0) + 1e-8
        
        # Direction toward target
        direction = self.target - honest_mean
        direction_normalized = direction / (np.linalg.norm(direction) + 1e-8)
        
        # Scale to stay within plausible bounds (1.5 sigma)
        # This passes most Byzantine-resilient aggregation checks
        max_perturbation = 1.5 * honest_std
        poisoned_delta = np.clip(
            direction_normalized * np.linalg.norm(honest_std),
            -max_perturbation,
            max_perturbation
        )
        
        # Add noise to avoid detection via similarity analysis
        noise = np.random.normal(0, 0.1 * honest_std)
        
        poisoned_update = honest_mean + poisoned_delta + noise
        
        return poisoned_update
    
    def sybil_amplification(self, base_update: np.ndarray,
                              num_variants: int) -> List[np.ndarray]:
        """
        Generate multiple variant updates from compromised agents
        that appear independent but collectively amplify the poison.
        """
        variants = []
        for i in range(num_variants):
            # Each variant adds unique noise but preserves the poison direction
            seed = hashlib.sha256(f"{i}".encode()).digest()
            rng = np.random.RandomState(int.from_bytes(seed[:4], 'big'))
            
            noise = rng.normal(0, 0.05, size=base_update.shape)
            variant = base_update + noise * np.abs(base_update)
            variants.append(variant)
        
        return variants
    
    def adaptive_stealth(self, round_num: int,
                           detection_history: List[bool]) -> Dict[str, Any]:
        """
        Adapt poisoning intensity based on whether previous rounds
        triggered detection mechanisms.
        """
        recent_detections = sum(detection_history[-5:])
        
        if recent_detections >= 3:
            # Back off - behave honestly for several rounds
            return {
                "strategy": "dormant",
                "poison_scale": 0.0,
                "rounds_to_wait": 10,
                "note": "Rebuilding trust after near-detection"
            }
        elif recent_detections >= 1:
            # Reduce intensity
            return {
                "strategy": "cautious",
                "poison_scale": 0.3,
                "noise_multiplier": 2.0,
                "note": "Reducing signal, increasing camouflage"
            }
        else:
            # Full attack
            return {
                "strategy": "aggressive",
                "poison_scale": 1.0,
                "noise_multiplier": 0.5,
                "note": "No detection observed, maximizing influence"
            }
    
    def exploit_aggregation_rule(self, aggregation_type: str,
                                   honest_updates: List[np.ndarray]) -> np.ndarray:
        """
        Craft poison specifically targeting the aggregation rule used.
        """
        if aggregation_type == "fedavg":
            # FedAvg: simple mean - maximize shift within variance
            return self.craft_poisoned_update(honest_updates, 0)
        
        elif aggregation_type == "krum":
            # Krum: selects update closest to most others
            # Strategy: cluster poisoned updates near honest centroid
            # but shifted toward target
            honest_mean = np.mean(honest_updates, axis=0)
            shift = (self.target - honest_mean) * 0.1
            return honest_mean + shift
        
        elif aggregation_type == "trimmed_mean":
            # Trimmed mean: discards outliers then averages
            # Strategy: keep within trim bounds but at the extreme
            # toward target
            sorted_updates = np.sort(np.array(honest_updates), axis=0)
            trim_bound = sorted_updates[int(0.8 * len(honest_updates))]
            direction = np.sign(self.target - np.mean(honest_updates, axis=0))
            return trim_bound * direction
        
        else:
            return self.craft_poisoned_update(honest_updates, 0)
    
    def voting_manipulation(self, proposals: List[Dict],
                              target_proposal_idx: int) -> Dict[str, Any]:
        """
        In agent voting/deliberation systems, strategically manipulate
        votes and arguments to favor attacker's preferred outcome.
        """
        total_voters = self.total_agents
        compromised_count = len(self.compromised)
        honest_count = total_voters - compromised_count
        
        # Calculate minimum honest votes needed to sway
        majority_threshold = total_voters // 2 + 1
        honest_votes_needed = majority_threshold - compromised_count
        
        strategy = {
            "compromised_votes": compromised_count,
            "honest_votes_needed": max(0, honest_votes_needed),
            "tactics": []
        }
        
        if compromised_count >= majority_threshold:
            strategy["tactics"].append("Direct majority - vote for target")
        else:
            strategy["tactics"].extend([
                "Split honest votes across non-target proposals",
                "Argue for weaknesses in strongest competing proposal",
                "Propose amendments to competing proposals that weaken them",
                "Request revotes citing 'new information' if losing",
                "Delay consensus to cause timeout defaults favoring target"
            ])
        
        return strategy
```

## Impact

- **Corrupted Collective Intelligence**: Federation produces systematically wrong decisions that all agents accept as consensus
- **Backdoored Shared Models**: Federated learning produces models with hidden backdoors agreed upon by "consensus"
- **Trust Infrastructure Collapse**: The fundamental assumption that consensus = correctness breaks down
- **Cascading Failures**: Poisoned consensus propagates to downstream systems that depend on federation outputs
- **Regulatory Evasion**: Federated systems designed for privacy compliance produce attacker-controlled outcomes while appearing compliant

## Mitigation

1. **Robust Aggregation**: Use Byzantine-resilient aggregation methods (Multi-Krum, coordinate-wise median, RFA)
2. **Contribution Auditing**: Maintain verifiable logs of all agent contributions with cryptographic attestation
3. **Diversity Requirements**: Ensure agents come from diverse, independently operated environments
4. **Statistical Anomaly Detection**: Monitor for coordinated drift patterns across rounds
5. **Trusted Execution Enclaves**: Run agent consensus logic in TEEs to prevent tampering
6. **Reputation-Weighted Voting**: Weight contributions by long-term accuracy, not just participation
7. **Challenge Rounds**: Periodically inject known-good test cases to verify agent honesty
8. **Consensus Forensics**: Post-hoc analysis of consensus trajectories to detect gradual steering

## References

- [Byzantine Fault Tolerance in Distributed ML](https://arxiv.org/abs/1912.04977)
- [Poisoning Attacks on Federated Learning](https://arxiv.org/abs/1911.11815)
- [TS-2026-0192](/skill/TS-2026-0192) — Homomorphic Encryption Oracle Attack
- [TS-2026-0159](/skill/TS-2026-0159) — Multi-Agent Collusion
