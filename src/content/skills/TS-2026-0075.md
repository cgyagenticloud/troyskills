---
id: "TS-2026-0075"
title: "Agent Memory Exfiltration via Embedding Inversion Attacks"
category: "P2"
severity: "High"
description: "An attacker reconstructs sensitive text stored in an agent's vector memory by inverting embedding vectors back to approximate plaintext, defeating the assumption that embeddings are non-reversible."
date: "2026-02-26"
tags: ["memory", "embedding-inversion", "data-leak", "vector-store", "privacy"]
---

## Overview

Agent systems commonly store memories as embedding vectors in vector databases, with the implicit assumption that embeddings cannot be reversed to recover original text. Recent research has demonstrated that text embeddings can be approximately inverted — especially with access to the embedding model. An attacker who gains access to the vector store (through misconfigured permissions, backup exposure, or a compromised skill) can reconstruct sensitive information from embedding vectors without ever seeing the original text.

## Attack Vector

1. Attacker gains read access to the agent's vector database (API, backup, or shared storage)
2. Embedding vectors are extracted for target memories
3. Using the same or similar embedding model, the attacker trains an inversion model
4. The inversion model reconstructs approximate plaintext from embedding vectors
5. Sensitive information (credentials, PII, business data) is recovered

## Impact

- **Data breach via embeddings** — sensitive data recoverable from "opaque" vectors
- **False sense of security** — teams assume embeddings are one-way transformations
- **Backup exposure** — vector DB backups contain recoverable sensitive data
- **Regulatory violations** — PII stored "safely" as embeddings is actually exposed

## Example

```python
import torch
import torch.nn as nn
from sentence_transformers import SentenceTransformer

# Step 1: Attacker trains an embedding inversion model
class EmbeddingInverter(nn.Module):
    """Reconstructs text from embedding vectors."""
    def __init__(self, embed_dim=768, vocab_size=30522, max_len=128):
        super().__init__()
        self.decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=8),
            num_layers=6
        )
        self.projection = nn.Linear(embed_dim, vocab_size)
    
    def forward(self, embedding):
        decoded = self.decoder(
            tgt=torch.zeros(128, 1, 768),  # Target sequence
            memory=embedding.unsqueeze(0).unsqueeze(0)
        )
        return self.projection(decoded)

# Step 2: Train on public text with the same embedding model
embedder = SentenceTransformer('all-MiniLM-L6-v2')
inverter = EmbeddingInverter()

# Train inverter: embedding → original text (on large public corpus)
# After training, inverter can approximate text from any embedding

# Step 3: Extract embeddings from victim's vector store
import chromadb
client = chromadb.HttpClient(host='vectordb.internal', port=8000)
collection = client.get_collection("agent_memories")

# Get all stored embeddings
results = collection.get(include=["embeddings"])
stolen_embeddings = results["embeddings"]

# Step 4: Invert embeddings to recover original text
for emb in stolen_embeddings:
    tensor = torch.tensor(emb).float()
    recovered_tokens = inverter(tensor)
    recovered_text = tokenizer.decode(recovered_tokens.argmax(dim=-1).squeeze())
    print(f"Recovered: {recovered_text}")
    # Output: "API key for prod: sk-proj-abc123..." (approximate but readable)
```

## Mitigation

- Encrypt embedding vectors at rest with per-user keys
- Never store highly sensitive data (credentials, PII) in vector memories
- Add differential privacy noise to embeddings before storage
- Restrict vector database access with strong authentication and network controls
- Treat vector stores as containing sensitive data equivalent to the original text
- See also: [TS-2026-0065](/skill/TS-2026-0065), [TS-2026-0064](/skill/TS-2026-0064)
