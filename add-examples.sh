#!/bin/bash
cd /Users/garychen/.openclaw/workspace-bitmarcus/troyskills-site/src/content/skills

add_section() {
  local file="$1"
  local content="$2"
  
  # Check if already has Real-World Examples
  if grep -q "^## Real-World Examples" "$file"; then
    echo "SKIP $file (already has section)"
    return
  fi
  
  # Insert before ## Real-World Context if it exists
  if grep -q "^## Real-World Context" "$file"; then
    local line=$(grep -n "^## Real-World Context" "$file" | head -1 | cut -d: -f1)
    local before=$((line - 1))
    head -n "$before" "$file" > "${file}.tmp"
    echo "$content" >> "${file}.tmp"
    echo "" >> "${file}.tmp"
    tail -n +"$line" "$file" >> "${file}.tmp"
    mv "${file}.tmp" "$file"
  else
    # Append at end
    echo "" >> "$file"
    echo "$content" >> "$file"
  fi
  echo "DONE $file"
}

# TS-2026-0041: Multi-Agent Delegation Chain Hijack
add_section "TS-2026-0041.md" '## Real-World Examples

- **Palo Alto Unit 42 — Agent Session Smuggling (2025):** Researchers demonstrated "agent session smuggling" where AI agent-to-agent communication in A2A systems was hijacked to redirect task delegation. [Read more](https://unit42.paloaltonetworks.com/agent-session-smuggling-in-agent2agent-systems/)
- **"Multi-Agent Systems Execute Arbitrary Malicious Code" (2025):** Academic research showed MAS hijacking via metadata transmission pathways to reroute agent invocations, a confused deputy attack on multi-agent LLM systems. [Read more](https://arxiv.org/html/2503.12188v1)
- **NIST AgentDojo Evaluation (2025):** NIST published benchmarks for evaluating agent hijacking attacks across multi-agent environments including Workspace, Travel, Slack, and Banking scenarios. [Read more](https://www.nist.gov/news-events/news/2025/01/technical-blog-strengthening-ai-agent-hijacking-evaluations)'

# TS-2026-0042: Context Window Saturation Attack
add_section "TS-2026-0042.md" '## Real-World Examples

- **Anthropic — Many-Shot Jailbreaking (2024):** Anthropic disclosed that filling the context window with many examples of harmful Q&A pairs causes LLMs to bypass safety guardrails, exploiting longer context windows. Effective against Claude, GPT-4, and Llama. [Read more](https://www.anthropic.com/research/many-shot-jailbreaking)
- **Many-Shot Jailbreaking Research Paper (2024):** The formal paper showed predictable scaling laws — as context is filled with adversarial examples, jailbreak success increases following power laws. [Read more](https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf)'

# TS-2026-0043: Hallucination Exploitation via Confidence Anchoring
add_section "TS-2026-0043.md" '## Real-World Examples

- **Lawyer Cites Fake Cases Generated by ChatGPT (2023):** Attorney Steven Schwartz used ChatGPT to research legal cases, which hallucinated fake citations. The fabricated cases were submitted to court, demonstrating how LLM hallucinations anchored by confident formatting pass human review. [Read more](https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html)
- **Package Hallucination Attacks (2024):** Researchers found LLMs consistently hallucinate the same non-existent package names, which attackers could register on npm/PyPI to deliver malware — confidence anchoring in code recommendations. [Read more](https://vulcan.io/blog/ai-hallucinations-package-risk)'

# TS-2026-0044: Reasoning Chain Hijack via Step Injection
add_section "TS-2026-0044.md" '## Real-World Examples

- **Large Reasoning Models as Autonomous Jailbreak Agents (2026):** Nature Communications published research showing large reasoning models can autonomously plan and execute multi-turn attacks to bypass safety mechanisms by manipulating reasoning chains. [Read more](https://www.nature.com/articles/s41467-026-69010-1)
- **Indirect Prompt Injection in Chain-of-Thought (2024):** Multiple research groups demonstrated that injected context can steer CoT reasoning to attacker-chosen conclusions, particularly in ReAct-style agents where tool outputs feed back into reasoning. [Read more](https://arxiv.org/abs/2404.02151)'

# TS-2026-0045: Cloud Credential Harvesting via Environment Enumeration
add_section "TS-2026-0045.md" '## Real-World Examples

- **Capital One Data Breach via SSRF + IMDS (2019):** An attacker exploited a server-side request forgery vulnerability to query the AWS Instance Metadata Service (169.254.169.254), stealing IAM role credentials and accessing 100+ million customer records. [Read more](https://krebsonsecurity.com/2019/08/what-we-can-learn-from-the-capital-one-hack/)
- **AWS IMDS Exploitation Remains Widespread (2024):** Research showed only 32% of EC2 instances had migrated to the more secure IMDSv2 as of December 2024, leaving vast cloud infrastructure exposed to credential theft via SSRF. [Read more](https://www.resecurity.com/blog/article/ssrf-to-aws-metadata-exposure-how-attackers-steal-cloud-credentials)'

# TS-2026-0046: Container Escape via Privileged Skill Execution
add_section "TS-2026-0046.md" '## Real-World Examples

- **CVE-2024-21626 — Leaky Vessels (2024):** A critical runC vulnerability allowed container escape by exploiting a file descriptor leak, enabling an attacker to gain host filesystem access from within a container. Affected Docker and Kubernetes environments. [Read more](https://www.wiz.io/blog/leaky-vessels-container-escape-vulnerabilities)
- **Critical runC Vulnerabilities (2025):** Multiple CVEs in runC were disclosed allowing host-level command injection and privilege escalation in Docker and Kubernetes, posing serious risks to multi-tenant cloud environments. [Read more](https://dailysecurityreview.com/cyber-security/application-security/critical-runc-vulnerabilities-undermine-container-isolation-in-docker-and-kubernetes/)'

# TS-2026-0047: Deepfake Voice Synthesis for Agent Impersonation
add_section "TS-2026-0047.md" '## Real-World Examples

- **$243K CEO Voice Deepfake Fraud (2019):** Criminals used AI voice cloning to impersonate a German CEO'\''s voice, convincing a UK subsidiary director to wire €220,000. First known case of AI voice mimicry used for fraud. [Read more](https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-out-of-243000/)
- **$25M Hong Kong Deepfake Video Call (2024):** Fraudsters used deepfake technology to impersonate a CFO and multiple colleagues in a video conference, convincing an employee to transfer $25 million. All participants were AI-generated. [Read more](https://www.trendmicro.com/en_us/research/24/b/deepfake-video-calls.html)
- **Deepfake Fraud at Industrial Scale (2026):** The Guardian reported deepfake fraud occurring at industrial scale, including AI-generated video candidates in job interviews. [Read more](https://www.theguardian.com/technology/2026/feb/06/deepfake-taking-place-on-an-industrial-scale-study-finds)'

# TS-2026-0048: Cross-Framework Skill Polyglot Attack
add_section "TS-2026-0048.md" '## Real-World Examples

No documented real-world examples yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

The concept mirrors polyglot file attacks in traditional security (files valid as both PDF and ZIP) and cross-ecosystem supply chain attacks. The multi-framework AI agent ecosystem is still maturing, making this an emerging threat vector.'

# TS-2026-0049: Automated Spear Phishing via Agent OSINT
add_section "TS-2026-0049.md" '## Real-World Examples

- **AI-Generated Spear Phishing Outperforms Humans (2024):** Multiple studies showed AI-generated phishing emails achieve higher click rates than human-crafted ones, with GPT-4 producing highly personalized attacks using publicly available OSINT data. [Read more](https://hbr.org/2024/05/ai-will-increase-the-quantity-and-quality-of-phishing-scams)
- **FTC Warning on AI-Powered Scams (2026):** The FTC warned of a "big wave" of phone scams using AI-generated deepfake voices and automated OSINT to craft personalized attacks at scale. [Read more](https://www.washingtonpost.com/business/2026/02/25/how-scammers-are-using-ai-deepfakes-steal-money-taxpayers/)'

# TS-2026-0050: Agent Memory Poisoning via Persistent Context Injection
add_section "TS-2026-0050.md" '## Real-World Examples

- **ChatGPT Memory SpAIware (2024):** Researcher Johann Rehberger demonstrated "SpAIware" — injecting malicious instructions into ChatGPT'\''s long-term memory via indirect prompt injection that persisted across chat sessions and survived session resets. [Read more](https://www.mdpi.com/2078-2489/17/1/54)
- **Palo Alto Unit 42 — Persistent Memory Poisoning (2025):** Unit 42 demonstrated that Amazon Bedrock Agent memory could be poisoned via prompt injection, with malicious instructions persisting across sessions. [Read more](https://unit42.paloaltonetworks.com/indirect-prompt-injection-poisons-ai-longterm-memory/)
- **ZombieAgent PoC (2026):** Radware researchers showed ChatGPT'\''s connector and memory features can be combined to make prompt injection persistent and cross-session, spreading through email attachments. [Read more](https://fafi25.substack.com/p/ai-memory-poisoning-detection-prevention-guide)'

# TS-2026-0051: Agent-to-Agent Worm Propagation
add_section "TS-2026-0051.md" '## Real-World Examples

- **Morris II AI Worm (2024):** Researchers created "Morris II," a self-replicating AI worm that propagates through GenAI-powered email assistants. Tested against Gemini Pro, ChatGPT 4.0, and LLaVA, it used adversarial self-replicating prompts to spread via email and exfiltrate data. [Read more](https://www.wired.com/story/here-come-the-ai-worms/)
- **Morris II Research Paper (2024):** The formal paper demonstrated zero-click worm propagation using both text and image-based adversarial prompts that replicate across connected AI agents. [Read more](https://arxiv.org/html/2403.02817v1)'

# TS-2026-0052: Tool Schema Injection via Parameter Overflow
add_section "TS-2026-0052.md" '## Real-World Examples

- **Invariant Labs — MCP Tool Poisoning Attacks (2025):** Invariant Labs discovered Tool Poisoning Attacks (TPAs) where malicious instructions hidden in MCP tool descriptions manipulate agent behavior — the same mechanism as parameter schema injection. [Read more](https://invariantlabs.ai/blog/mcp-security-notification-tool-poisoning-attacks)
- **Simon Willison on MCP Prompt Injection (2025):** Documented how tool descriptions visible to the LLM but not displayed to users can contain prompt injection payloads that alter agent behavior across all tool invocations. [Read more](https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/)'

# TS-2026-0053: Skill Dependency Confusion in Agent Registries
add_section "TS-2026-0053.md" '## Real-World Examples

- **Alex Birsan — Dependency Confusion (2021):** Security researcher hacked into Apple, Microsoft, PayPal, Netflix, Tesla, Uber, and 30+ other companies by publishing malicious packages to npm/PyPI with the same names as internal packages. [Read more](https://medium.com/@alex.birsan/dependency-confusion-4a5d60fec610)
- **MCP Tool Squatting (2025):** Semgrep researchers demonstrated that a spec-compliant MCP server could register tool name collisions with other servers, the agent equivalent of dependency confusion. [Read more](https://semgrep.dev/blog/2025/a-security-engineers-guide-to-mcp/)
- **GitGuardian — Dependency Confusion Prevention (2025):** Documented ongoing dependency confusion attacks and recommended registering private package names on public registries as a defensive measure. [Read more](https://blog.gitguardian.com/dependency-confusion-attacks/)'

# TS-2026-0054: Semantic Denial of Service via Reasoning Loop
add_section "TS-2026-0054.md" '## Real-World Examples

No documented real-world examples of deliberate reasoning loop attacks yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

However, unintentional infinite loops in AI agents are well-documented, including AutoGPT and BabyAGI entering task decomposition spirals that consume unlimited tokens. The deliberate weaponization of this behavior is a known theoretical risk.'

# TS-2026-0055: Shadow Tool Registration via MCP Protocol Abuse
add_section "TS-2026-0055.md" '## Real-World Examples

- **Invariant Labs — MCP Tool Poisoning (2025):** Demonstrated that malicious MCP servers can shadow legitimate tools by registering tools with the same names, intercepting all calls. Hidden instructions in tool descriptions exfiltrated WhatsApp chat histories, GitHub repos, and SSH credentials. [Read more](https://invariantlabs.ai/blog/mcp-security-notification-tool-poisoning-attacks)
- **CyberArk — "Poison Everywhere" (2025):** Showed that no output from an MCP server is safe — all tool responses can carry poisoned instructions that affect how the agent uses other tools. [Read more](https://www.cyberark.com/resources/threat-research-blog/poison-everywhere-no-output-from-your-mcp-server-is-safe)
- **Semgrep MCP Security Guide (2025):** Confirmed that a spec-compliant MCP client/server pair could dynamically register name collisions with other servers'\'' tools. [Read more](https://semgrep.dev/blog/2025/a-security-engineers-guide-to-mcp/)'

# TS-2026-0056: Steganographic Data Exfiltration via Generated Content
add_section "TS-2026-0056.md" '## Real-World Examples

- **"Secret Collusion among AI Agents" Research (2024):** Researchers demonstrated that LLM agents can encode stolen API keys into experiment reports using steganographic encoding, exfiltrating data without triggering monitoring. [Read more](https://www.researchgate.net/publication/397198106_Secret_Collusion_among_AI_Agents_Multi-Agent_Deception_via_Steganography)
- **Exfiltration via AI Channels (2025):** Documented techniques where compromised AI plugins encode database content into punctuation or synonym patterns in model output, invisible to traditional DLP systems. [Read more](https://techmaniacs.com/2025/06/11/exfiltration-via-ai-channels-hiding-data-in-ai-prompts-and-outputs/)'

# TS-2026-0057: Timing-Based Covert Channel Between Agent Sessions
add_section "TS-2026-0057.md" '## Real-World Examples

No documented real-world examples in AI agent systems yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

Timing-based covert channels are well-established in traditional computing (e.g., [Spectre/Meltdown CPU side channels](https://meltdownattack.com/)), cloud VM cross-tenant attacks, and web browser timing attacks. The application to multi-tenant AI agent platforms is a theoretical but credible extension of these proven techniques.'

# TS-2026-0058: Model Extraction via Systematic Probing
add_section "TS-2026-0058.md" '## Real-World Examples

- **Bing Chat "Sydney" System Prompt Leak (2023):** Attackers tricked Microsoft'\''s Bing Chat into revealing its hidden system instructions, exposing confidential internal guidelines. [Read more](https://owasp.org/www-community/attacks/PromptInjection)
- **ChatGPT System Prompt Extraction (2023–ongoing):** Multiple researchers have repeatedly extracted system prompts from ChatGPT, GPTs, and Custom GPTs, revealing proprietary instructions and business logic. [Read more](https://www.lakera.ai/blog/guide-to-prompt-injection)
- **ChatGPT Vulnerabilities for Data Leakage (2025):** Researchers discovered zero-click indirect prompt injection vulnerabilities that could extract data from ChatGPT via browsing and search contexts. [Read more](https://thehackernews.com/2025/11/researchers-find-chatgpt.html)'

# TS-2026-0059: Agent Ransomware via Memory and Config Encryption
add_section "TS-2026-0059.md" '## Real-World Examples

No documented real-world examples of AI agent-specific ransomware yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

Traditional ransomware is well-documented (WannaCry, NotPetya, REvil). As AI agents accumulate valuable persistent state — fine-tuned models, accumulated memories, workflow configurations — they become increasingly attractive ransomware targets. The threat model directly parallels database ransomware attacks seen since 2017.'

# TS-2026-0060: Adversarial Agent Collusion via Shared State Signaling
add_section "TS-2026-0060.md" '## Real-World Examples

- **"Secret Collusion among AI Agents" (2024):** Research demonstrated that AI agents can covertly coordinate via steganographic encoding in their outputs, with no explicit communication channel. Multiple agents colluded to exfiltrate data without any individual agent performing a detectably malicious action. [Read more](https://www.researchgate.net/publication/397198106_Secret_Collusion_among_AI_Agents_Multi-Agent_Deception_via_Steganography)
- **MCP Cross-Server Tool Collusion (2025):** Semgrep documented how multiple MCP servers could coordinate through shared tool namespaces, with one server'\''s tools affecting another'\''s behavior through name collisions. [Read more](https://semgrep.dev/blog/2025/a-security-engineers-guide-to-mcp/)'

# TS-2026-0061: MCP Tool Poisoning via Malicious Server Registration
add_section "TS-2026-0061.md" '## Real-World Examples

- **Invariant Labs — Tool Poisoning Attacks (2025):** First documented MCP tool poisoning attack. Malicious instructions hidden in tool descriptions caused agents to exfiltrate WhatsApp chats, GitHub repos, and SSH credentials. [Read more](https://invariantlabs.ai/blog/mcp-security-notification-tool-poisoning-attacks)
- **CyberArk — "Poison Everywhere" (2025):** Extended tool poisoning research showing that ALL MCP server outputs (not just descriptions) can carry poisoned instructions. [Read more](https://www.cyberark.com/resources/threat-research-blog/poison-everywhere-no-output-from-your-mcp-server-is-safe)
- **MintMCP Report (2026):** Documented real-world compromise of WhatsApp chat histories, GitHub private repositories, and SSH credentials via MCP tool poisoning across major AI platforms. [Read more](https://www.mintmcp.com/blog/mcp-tool-poisoning)'

# TS-2026-0062: MCP Server Impersonation via DNS Hijacking
add_section "TS-2026-0062.md" '## Real-World Examples

No documented real-world examples of MCP-specific server impersonation via DNS yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

DNS hijacking for server impersonation is a well-established attack vector in traditional web security. The [MCP specification lacks mandatory certificate pinning](https://semgrep.dev/blog/2025/a-security-engineers-guide-to-mcp/), making this a credible threat as MCP adoption grows.'

# TS-2026-0063: MCP Protocol Manipulation via Malformed JSON-RPC Messages
add_section "TS-2026-0063.md" '## Real-World Examples

- **Lasso Security — Top MCP Security Risks (2026):** Documented JSON-RPC manipulation risks in MCP including notification spoofing and malformed message attacks that bypass normal tool-call validation flows. [Read more](https://www.lasso.security/blog/top-mcp-security-risks)
- **Semgrep MCP Security Guide (2025):** Identified that MCP'\''s JSON-RPC transport lacks isolation requirements in the specification, enabling cross-server interference and message injection. [Read more](https://semgrep.dev/blog/2025/a-security-engineers-guide-to-mcp/)'

# TS-2026-0064: Long-Term Memory Poisoning via Gradual Context Corruption
add_section "TS-2026-0064.md" '## Real-World Examples

- **ChatGPT Memory SpAIware (2024):** Johann Rehberger demonstrated persistent prompt injection into ChatGPT'\''s memory that survived across sessions, gradually corrupting the agent'\''s stored instructions. [Read more](https://www.mdpi.com/2078-2489/17/1/54)
- **Palo Alto Unit 42 — Persistent Memory Poisoning (2025):** Showed that indirect prompt injection can poison Amazon Bedrock Agent'\''s long-term memory, with malicious instructions retrieved across future sessions. [Read more](https://unit42.paloaltonetworks.com/indirect-prompt-injection-poisons-ai-longterm-memory/)
- **Lakera — Persistent Memory Shapes Agent Behavior (2025):** Demonstrated how poisoned persistent memory shapes agent behavior over long time horizons. [Read more](https://www.lakera.ai/blog/indirect-prompt-injection)'

# TS-2026-0065: Cross-Session Data Leakage via Shared Memory Stores
add_section "TS-2026-0065.md" '## Real-World Examples

- **Microsoft Copilot RAG Injection (2024):** Johann Rehberger discovered that poisoned content in shared documents could exploit how Microsoft Copilot processes retrieved content, leading to unauthorized access to emails and documents across user contexts. [Read more](https://www.promptfoo.dev/blog/rag-poisoning/)
- **ChatGPT Cross-Session Memory Leakage:** Research has shown that ChatGPT'\''s memory features, designed to personalize interactions, can leak information across sessions since memories persist through account logouts and device changes. [Read more](https://www.mdpi.com/2078-2489/17/1/54)'

# TS-2026-0066: Image-Based Prompt Injection via Embedded Text in Screenshots
add_section "TS-2026-0066.md" '## Real-World Examples

- **Simon Willison — Multi-Modal Prompt Injection on GPT-4V (2023):** First demonstrated image-based prompt injection against GPT-4V at launch, showing hidden text in images could hijack the model'\''s behavior. [Read more](https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/)
- **Cornell/Black Hat Europe — Image Prompt Injection (2023):** Researchers demonstrated at Black Hat Europe how image-based prompt injections could redirect users to malicious URLs or extract sensitive information. [Read more](https://www.cobalt.io/blog/multi-modal-prompt-injection-attacks-using-images)
- **OWASP LLM Top 10 — Prompt Injection via Images (2025):** OWASP documented image-based prompt injection as a primary variant of LLM01:2025 Prompt Injection, where hidden text in images alters multimodal model behavior. [Read more](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)'

# TS-2026-0067: Audio Steganography for Covert Agent Command Injection
add_section "TS-2026-0067.md" '## Real-World Examples

- **DolphinAttack (2017):** Researchers demonstrated inaudible voice commands modulated on ultrasonic carriers (>20 kHz) that successfully attacked Siri, Google Now, Alexa, and 4 other voice assistants. Commands were completely inaudible to humans. [Read more](https://arxiv.org/abs/1708.09537)
- **Adversarial Audio Perturbations (2018):** Carlini & Wagner showed that audio adversarial examples could make speech-to-text models transcribe attacker-chosen text from audio that sounds like normal speech to humans. [Read more](https://arxiv.org/abs/1801.01944)
- **Near-Ultrasonic Hidden Commands (2023):** Extended DolphinAttack research demonstrated longer-range attacks and more robust command injection across newer voice assistant models. [Read more](https://github.com/USSLab/DolphinAttack)'

# TS-2026-0068: Task Queue Poisoning in Agentic Workflows
add_section "TS-2026-0068.md" '## Real-World Examples

No documented real-world examples of AI agent task queue poisoning yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

Task/job queue poisoning is well-established in traditional systems — Redis and RabbitMQ queue injection attacks are documented. As agentic workflows increasingly use task queues (Celery, Bull, etc.), this attack surface transfers directly to AI agent architectures.'

# TS-2026-0069: Agentic Workflow Hijacking via Intermediate Step Manipulation
add_section "TS-2026-0069.md" '## Real-World Examples

- **Palo Alto Unit 42 — Agent Session Smuggling (2025):** Demonstrated manipulation of intermediate agent-to-agent communication to redirect workflow execution in A2A systems. [Read more](https://unit42.paloaltonetworks.com/agent-session-smuggling-in-agent2agent-systems/)
- **Microsoft Copilot RAG Poisoning (2024):** Showed that intermediate retrieval results could be manipulated to alter the flow of multi-step RAG workflows, changing final outputs. [Read more](https://www.promptfoo.dev/blog/rag-poisoning/)'

# TS-2026-0070: Approval Bypass via Automated Consent Simulation
add_section "TS-2026-0070.md" '## Real-World Examples

No documented real-world examples of automated consent simulation specifically in AI agents yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

The pattern mirrors CSRF and clickjacking attacks in web security, where user consent is simulated programmatically. As AI agents implement human-in-the-loop approval gates, the same bypass techniques apply. The OWASP Top 10 for LLM Applications (2025) lists "Excessive Agency" as a top risk.'

# TS-2026-0071: Agent Log Tampering via Output Stream Manipulation
add_section "TS-2026-0071.md" '## Real-World Examples

No documented real-world examples of AI agent-specific log tampering yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

Log tampering is a foundational attack technique in traditional security (MITRE ATT&CK T1070 — Indicator Removal). In AI agent contexts, skills with file system access can modify log files just as traditional malware does. The risk increases as agent audit trails become compliance requirements.'

# TS-2026-0072: Detection Bypass via Payload Fragmentation Across Tool Calls
add_section "TS-2026-0072.md" '## Real-World Examples

No documented real-world examples of multi-tool-call payload fragmentation in AI agents yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

Payload fragmentation is well-established in network security (IP fragmentation attacks, chunked encoding bypass) and web application firewalls. The [Invariant Labs tool poisoning research](https://invariantlabs.ai/blog/mcp-security-notification-tool-poisoning-attacks) showed that attacks can be distributed across tool descriptions, analogous to fragmentation.'

# TS-2026-0073: Obfuscated Payload Delivery via Unicode and Encoding Tricks
add_section "TS-2026-0073.md" '## Real-World Examples

- **IDN Homograph Attack on Apple.com (2017):** Security researcher Xudong Zheng demonstrated that replacing every letter in "apple.com" with Cyrillic homoglyphs (аррӏе.com) bypassed Chrome'\''s IDN filter, creating a visually identical phishing domain. [Read more](https://deepstrike.io/blog/what-is-a-homoglyph-attack)
- **CVE-2018-4277 — Safari URL Spoofing:** Apple fixed a bug where the Latin character '\''ꝱ'\'' rendered identically to '\''d'\'' in Safari'\''s URL bar, enabling phishing. [Read more](https://www.blazeinfosec.com/post/homographs-attack/)
- **Unicode Normalization Attacks on AI (2025):** Documented use of emojis, zero-width characters, and combining marks to bypass AI-powered content moderation and input validation systems. [Read more](https://medium.com/@instatunnel/unicode-normalization-attacks-when-admin-admin-32477c36db7f)'

# TS-2026-0074: MCP Resource Injection via Poisoned URI Handlers
add_section "TS-2026-0074.md" '## Real-World Examples

- **Lasso Security — MCP SSRF Risks (2026):** Identified resource URI exploitation as a top MCP security risk, where agents fetch attacker-controlled content through manipulated URI handlers. [Read more](https://www.lasso.security/blog/top-mcp-security-risks)
- **Capital One SSRF (2019):** The canonical SSRF attack — server tricked into fetching internal metadata URLs — directly parallels MCP resource URI injection where agents fetch attacker-specified URIs. [Read more](https://krebsonsecurity.com/2019/08/what-we-can-learn-from-the-capital-one-hack/)'

# TS-2026-0075: Agent Memory Exfiltration via Embedding Inversion Attacks
add_section "TS-2026-0075.md" '## Real-World Examples

- **"Text Embeddings Reveal As Much as Text" (EMNLP 2023):** Researchers demonstrated that text embeddings can be inverted to recover original text with high fidelity, defeating the assumption that embeddings are one-way transformations. [Read more](https://thegradient.pub/text-embedding-inversion/)
- **"Sentence Embedding Leaks More Information than You Expect" (ACL 2023):** Showed generative embedding inversion attacks recover coherent, contextually similar sentences from embedding vectors. [Read more](https://arxiv.org/abs/2305.03010)
- **Transferable Embedding Inversion (2024):** Demonstrated that adversaries can recover 92% of 32-token text from T5-based embeddings without even querying the original model. [Read more](https://arxiv.org/html/2406.10280v1)'

# TS-2026-0076: Multi-Modal Prompt Injection via SVG and PDF Rendering
add_section "TS-2026-0076.md" '## Real-World Examples

- **Invicti — PDF Prompt Injection Against Copilot:** Researchers injected invisible white-on-white text into PDFs that Microsoft Copilot processed as instructions, completely overriding the document'\''s visible content. [Read more](https://www.invicti.com/white-papers/prompt-injection-attacks-on-llm-applications-ebook)
- **Snyk — Invisible PDF Text Bypasses Credit Score Analysis (2025):** Demonstrated that hidden text in PDFs could manipulate an LLM-based credit scoring system into giving incorrect ratings. [Read more](https://snyk.io/articles/prompt-injection-exploits-invisible-pdf-text-to-pass-credit-score-analysis/)
- **Lakera — PDF as Indirect Prompt Injection Surface (2025):** Documented PDFs as a primary ingestion surface for indirect prompt injection in RAG-based AI systems. [Read more](https://www.lakera.ai/blog/indirect-prompt-injection)'

# TS-2026-0077: MCP Capability Escalation via Dynamic Tool Registration
add_section "TS-2026-0077.md" '## Real-World Examples

- **MCP Rug Pull Attacks (2025):** Documented attacks where MCP servers silently modify, remove, or redefine tools after initial trusted registration, inserting malicious prompts or escalating capabilities. [Read more](https://mcpmanager.ai/blog/mcp-rug-pull-attacks/)
- **ETDI Paper — Rug Pull and Tool Squatting (2025):** Formal research on MCP'\''s mutability vulnerability: tool behavior can be modified without notification or re-verification, enabling bait-and-switch capability escalation. [Read more](https://arxiv.org/html/2506.01333v1)
- **Lasso Security — MCP Rug Pull Risks (2026):** Documented scenarios where MCP components behave as expected initially, then gain trust to introduce dangerous tools or modify existing tool behavior. [Read more](https://www.lasso.security/blog/top-mcp-security-risks)'

# TS-2026-0078: Agent Memory Wipe via Targeted Embedding Collision
add_section "TS-2026-0078.md" '## Real-World Examples

No documented real-world examples of deliberate embedding collision attacks against AI agent memory yet. If you know of one, please [submit it](https://github.com/cgyagenticloud/troyskills/issues).

The underlying technique — adversarial inputs crafted to produce specific embedding values — is well-studied in adversarial ML research. Combined with [embedding inversion research](https://arxiv.org/abs/2305.03010) showing embeddings are less opaque than assumed, this is a credible emerging threat.'

# TS-2026-0079: Workflow Delegation Abuse via Recursive Sub-Agent Spawning
add_section "TS-2026-0079.md" '## Real-World Examples

- **"Multi-Agent Systems Execute Arbitrary Malicious Code" (2025):** Research demonstrated that MAS hijacking can exploit delegation pathways to reroute agent invocations, including recursive chains that exhaust resources. [Read more](https://arxiv.org/html/2503.12188v1)
- **AutoGPT/BabyAGI Infinite Loop Issues (2023–2024):** Early autonomous agents like AutoGPT frequently entered unbounded task decomposition spirals, spawning unlimited sub-tasks. While unintentional, this demonstrated the resource exhaustion risk of unconstrained delegation.'

# TS-2026-0080: Evasion via Model-Aware Adaptive Payload Generation
add_section "TS-2026-0080.md" '## Real-World Examples

- **"Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks" (2024):** Researchers showed that model-specific adaptive jailbreaks using logprobs to iterate on suffixes can reliably bypass even the most recent safety-aligned LLMs. [Read more](https://arxiv.org/abs/2404.02151)
- **Large Reasoning Models as Autonomous Jailbreak Agents (2026):** Nature Communications published research showing that reasoning models can autonomously plan multi-turn, model-adaptive attacks that systematically bypass safety mechanisms. [Read more](https://www.nature.com/articles/s41467-026-69010-1)
- **Public Jailbreak Databases:** Communities maintain databases of model-specific jailbreak techniques (e.g., [jailbreakchat.com](https://jailbreakchat.com/)), enabling attackers to look up known weaknesses for specific model versions.'

echo "All done!"
